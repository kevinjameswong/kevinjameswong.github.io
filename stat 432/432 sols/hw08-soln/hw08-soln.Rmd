---
title: "STAT 432 Homework 08"
author: "Spring 2018 | Dalpiaz | UIUC"
date: 'Partial *Solution*'
output: 
  html_document: 
    theme: flatly
    toc: true
---

```{r set-options, include = FALSE}
# setting some default chunk options
# figures will be centered
# messages are suppressed
# warnings are suppressed
knitr::opts_chunk$set(fig.align = "center", message = FALSE, warning = FALSE,
                      cache = TRUE, autodep = TRUE)
```

***

What follows are *brief* thoughts on each of the three tasks.

```{r, warning = FALSE, message = FALSE}
# suggested packages
library(MASS)
library(caret)
library(tidyverse)
library(knitr)
library(kableExtra)
library(mlbench)
library(ISLR)
library(ellipse)
library(randomForest)
library(gbm)
library(glmnet)
library(rpart)
library(rpart.plot)
library(klaR)
library(gam)
library(e1071)
# feel free to use additional packages
```

***

# Classification

## Introduction

```{r}
class_trn = read_csv("https://daviddalpiaz.github.io/stat432sp18/hw/hw08/class-trn.csv")
class_tst = read_csv("https://daviddalpiaz.github.io/stat432sp18/hw/hw08/class-tst.csv")
```

```{r}
accuracy = function(actual, predicted) {
  mean(actual == predicted)
}
```

Here have have all numeric predictors and a categorical response, which we coerce to be a factor.

```{r}
class_trn$y = factor(class_trn$y)
```

```{r}
table(class_trn$y)
```

The response variable is `y` and takes values `Zero` and `One`. (And is a factor variable.)

Our goal is to classify this response as well as possible. We can see above that there is a slightly higher proportion of `Zero` in the training data.

The following plots will explore the relationship of the ten feature variables with the response variable.

```{r, echo = FALSE, fig.height = 7, fig.width = 12}
featurePlot(x = class_trn[ ,2:11], y = class_trn$y, plot = "density",
            scales = list(x = list(relation = "free"), y = list(relation = "free")),
            adjust = 1.5, pch = "|", layout = c(5, 2),
            auto.key = list(columns = 2))
```

Based on the above plots we can see that `X6`, `X7`, and `X8` show promise as predictors, however, they would each need two cutoffs, a non-linear boundary.

```{r, echo = FALSE, fig.height = 6, fig.width = 6}
featurePlot(x = class_trn[, 2:6], y = class_trn$y,
            plot = "pairs", auto.key = list(columns = 2))
```

```{r, echo = FALSE, fig.height = 6, fig.width = 6}
featurePlot(x = class_trn[, 7:11], y = class_trn$y,
            plot = "pairs", auto.key = list(columns = 2))
```

The above two plots again reinforce that `X6`, `X7`, and `X8` should be good predictors. 

```{r, echo = FALSE, fig.height = 6, fig.width = 6}
featurePlot(x = class_trn[ , c(6 + 1, 7 + 1, 8 + 1)], y = class_trn$y,
            plot = "pairs", auto.key = list(columns = 2))
```

```{r, fig.height = 8, fig.width = 12, echo = FALSE}
class_tree = rpart(y ~ ., data = class_trn)
rpart.plot(class_tree, type = 4)
```

Here we fit a single classification tree. We see that it uses `X6`, `X7`, and `X8` for all of the splits.

```{r, fig.height = 8, fig.width = 12, results = 'hide', echo = FALSE}
class_boost = gbm((as.numeric(y) - 1) ~ ., data = class_trn, distribution = "bernoulli",
                  n.trees = 5000, interaction.depth = 4, shrinkage = 0.01)
summary(class_boost)
```

The above plot is a variable importance plot generated by fitting a boosted tree model. Again we see that `X6`, `X7`, and `X8` are clearly good predictors.

As a result, we will consider some models that use only `X6`, `X7`, and `X8`.

## Methods

We will consider five methods:

- Random Forest (`rf`)
- Regularized Discriminant Analysis (`rda`)
- Logistic Regression (`glm`)
- Penalized Logistic Regression, Elastic-Net (`glmnet`)
- Boosted Trees (`gbm`)

For each method we will consider different sets of features:

- `small`: Only `X6`, `X7`, and `X8`
- `int`: All possible interactions between `X6`, `X7`, and `X8`
- `poly`: All first and second degree terms using `X6`, `X7`, and `X8`
- `full`: All features.
- `huge`: All features with all possible two way interactions.

```{r}
small_form = formula(y ~ X6 + X7 + X8)
int_form   = formula(y ~ X6 * X7 * X8)
poly_form  = formula(y ~ X6 + X7 + X8 + I(X6 ^ 2) + I(X7 ^ 2) + I(X8 ^ 2))
full_form  = formula(y ~ .)
huge_form  = formula(y ~ . ^ 2)
```

`huge` will only be considered with Penalized Logistic Regression since it is a very large number of features. `poly` will only be considered with `glm`.

```{r}
cv_5 = trainControl(method = "cv", number = 5)
```

Tuning will be done using 5-fold cross-validation. (We could use OOB metrics for Random Forests, but here we sacrifice computation time for better comparison.)

```{r}
uin = 123456789
set.seed(uin)
```

```{r}
rf_small = train(small_form, data = class_trn, method = "rf", 
                 trControl = cv_5, tuneLength = 2)

rf_int   = train(int_form, data = class_trn, method = "rf", 
                 trControl = cv_5)

rf_full  = train(full_form, data = class_trn, method = "rf", 
                 trControl = cv_5)
```

Random Forests are fit above using `caret` with `method = "rf"`. For most we use the default training grid, which considers a low, medium and high `mtry`. We specify a training grid of length `2` for the small feature set to avoid a warning. (The default settings result in a tune length of 2 in this situation, since there are so few predictors.)

```{r}
rda_small = train(small_form, data = class_trn, method = "rda", 
                  trControl = cv_5)

rda_int   = train(int_form, data = class_trn, method = "rda", 
                  trControl = cv_5)

rda_full = train(full_form, data = class_trn, method = "rda", 
                 trControl = cv_5)
```

RDA is fit above using `caret` with `method = "rda"`. We simply use the default tuning grid, which will consider both LDA, QDA, as well as some other models.

```{r}
glm_small = train(small_form, data = class_trn, method = "glm", 
                  trControl = cv_5)

glm_poly  = train(poly_form, data = class_trn, method = "glm", 
                  trControl = cv_5)

glm_full  = train(full_form, data = class_trn, method = "glm", 
                  trControl = cv_5)
```

Logistic Regressions are fit above using `caret` with `method = "glm"` which with a factor response automatically uses `family = "binomial"`. There is nothing to tune, but we store the results for comparing cross-validated errors later. One of the models considers quadratic terms for predictors noted above.

```{r}
glmn_small = train(small_form, data = class_trn, method = "glmnet", 
                   trControl = cv_5, tuneLength = 10)

glmn_int   = train(int_form, data = class_trn, method = "glmnet", 
                   trControl = cv_5, tuneLength = 10)

glmn_full  = train(full_form, data = class_trn, method = "glmnet", 
                   trControl = cv_5, tuneLength = 10)

glmn_huge  = train(huge_form, data = class_trn, method = "glmnet", 
                   trControl = cv_5, tuneLength = 10)
```

Penalized Logistic Regressions are fit above using `caret` with `method = "glmnet"` which with a factor response automatically uses `family = "binomial"`. We simply specify a tuning grid length. `caret` will try 10 $\alpha$ values and a number of $\lambda$ values for each.

```{r}
gbm_grid  = expand.grid(interaction.depth = c(1:3), n.trees = 1:3 * 500, 
                        shrinkage = c(0.001, 0.01, 0.1), n.minobsinnode = 10)

gbm_small = train(small_form, data = class_trn, method = "gbm", 
                  verbose = FALSE, trControl = cv_5, tuneGrid = gbm_grid)

gbm_int   = train(int_form, data = class_trn, method = "gbm", 
                  verbose = FALSE, trControl = cv_5, tuneGrid = gbm_grid)

gbm_full  = train(full_form, data = class_trn, method = "gbm", 
                  verbose = FALSE, trControl = cv_5, tuneGrid = gbm_grid)
```

Boosted Trees are fit above using `caret` with `method = "gbm"`. We specify a somewhat large custom tuning grid, keeping `interaction.depth` low and using enough trees for the chosen `shrinkage` values.

## Results

```{r, echo = FALSE}
model_list_class = list(
  rf_small = rf_small,
  rf_int = rf_int,
  rf_full = rf_full,
  rda_small = rda_small,
  rda_int = rda_int,
  rda_full = rda_full,
  glm_small = glm_small,
  glm_poly = glm_poly,
  glm_full = glm_full,
  glmn_small = glmn_small,
  glmn_int = glmn_int,
  glmn_full = glmn_full,
  glmn_huge = glmn_huge,
  gbm_small = gbm_small,
  gbm_int = gbm_int,
  gbm_full = gbm_full
)
```

```{r, echo = FALSE}
cv_acc = rep(0, length(model_list_class))
for (i in seq_along(model_list_class)) {
  cv_acc[i] = max(model_list_class[[i]]$results$Accuracy)
}

sd_cv_acc = rep(0, length(model_list_class))
for (i in seq_along(model_list_class)) {
  sd_cv_acc[i] = model_list_class[[i]]$results$AccuracySD[which.max(model_list_class[[i]]$results$Accuracy)]
}

```

```{r, echo = FALSE}
results_class = data.frame(models = names(model_list_class), 
                           cv_acc = cv_acc,
                           sd_cv_acc = sd_cv_acc)
colnames(results_class) = c("Method + Features", "CV Acc", "SD CV Acc")
results_class = results_class[rev(order(results_class$`CV Acc`)), ]
rownames(results_class) = NULL

kable(results_class, format = "html", digits = 3) %>%
  kable_styling(bootstrap_options = "striped", full_width = FALSE) %>%
  row_spec(row = 3, bold = TRUE, color = "white", background = "lightgreen")
```


## Discussion

Based on the above cross-validated accuracies, we need to decide which to submit to the autograder. The most obvious is `gbm_int`, which is the boosted tree model using all possible interactions between selected features. However, it is a rather complicated model with a reasonably large standard deviation compared to other models.

`rda_small` also performs well, but is still a somewhat complicated model.

`glm_poly` is very promising as it is a very simple model with a high CV-accuracy. We will select this model.

```{r}
rda_small$bestTune
```

```{r}
gbm_int$bestTune
```

Since this was synthetic data, we can actually see the true data generating process.

```{r}
# define the data generating process
gen_class_data = function(n) {
  p = 10
  x = round(replicate(p, rnorm(n)), 3)
  z = -3 + 1 * x[, 6] ^ 2 + 1 * x[, 7] ^ 2 + 1 * x[, 8] ^ 2
  prob = exp(z) / (1 + exp(z))
  y = rbinom(length(z), 1, prob)
  y = ifelse(y == 1, "One", "Zero")
  data.frame(y = as.factor(y), x)
}
```

As a result, the following function is the best possible model for this data.

```{r}
f = function(predictors) {
  -3 + predictors$X6 ^ 2 + predictors$X7 ^ 2 + predictors$X8 ^ 2
}
```

This means that our chosen model was very close to the true model. (The signs are flipped due to the coding of the factor variable.)

```{r}
glm_poly$finalModel
```


```{r}
class_tst_y = read_csv("class-tst-y.csv") # this data not released
class_tst_y$y = factor(class_tst_y$y)
```

Thus the accuracy below is the best we could achieve on the test data.

```{r}
accuracy(ifelse(f(class_tst) > 0, "One", "Zero"), class_tst_y$y)
```

This means that several of the above models did very well. Results with a test accuracy above this value are the resulting of fitting to the noise of the test data.

```{r}
accuracy(predict(glm_poly, class_tst), class_tst_y$y)
```

***

# Regression

## Introduction

```{r, message = FALSE, warning = FALSE}
reg_trn = read_csv("https://daviddalpiaz.github.io/stat432sp18/hw/hw08/reg-trn.csv")
reg_tst = read_csv("https://daviddalpiaz.github.io/stat432sp18/hw/hw08/reg-tst.csv")
```

```{r}
rmse = function(actual, predicted) {
  sqrt(mean((actual - predicted) ^ 2))
}
```

Here we have a numeric response, and some categorical predictors that we coerce to be factors.

```{r}
# modify train
reg_trn$X21 = factor(reg_trn$X21)
reg_trn$X22 = factor(reg_trn$X22)

# modify test to match
reg_tst$X21 = factor(reg_tst$X21)
reg_tst$X22 = factor(reg_tst$X22)
```

```{r}
histogram(reg_trn$y)
```

The response variable is `y`. Our goal is to predict this response as well as possible.

The following plots will explore the relationship of the twenty feature variables with the response variable.

```{r, echo = FALSE, fig.height = 10, fig.width = 10}
theme1 = trellis.par.get()
theme1$plot.symbol$col = rgb(.2, .2, .2, .4)
theme1$plot.symbol$pch = 16
theme1$plot.line$col = rgb(1, 0, 0, .7)
theme1$plot.line$lwd = 2
trellis.par.set(theme1)
featurePlot(x = reg_trn[, -c(1, 22, 23)], y = reg_trn$y, plot = "scatter",
            type = c("p", "smooth"), layout = c(5, 4))
```

From the above plot, features `X1`, `X2`, `X3`, `X4` and `X6` show potential.

```{r, fig.height = 8, fig.width = 14, echo = FALSE}
reg_tree = rpart(y ~ ., data = reg_trn)
rpart.plot(reg_tree, type = 4)
```

Here we fit a single regression tree. We see that it uses `X3`, `X4`, `X6` and `X22` exclusively.

```{r, fig.height = 10, fig.width = 12, results = 'hide', echo = FALSE}
reg_boost = gbm(y ~ ., data = reg_trn, distribution = "gaussian",
                n.trees = 5000, interaction.depth = 4, shrinkage = 0.01)
summary(reg_boost)
```

The above plot is a variable importance plot generated by fitting a boosted tree model. Again we see that `X6` and `X22` are by far the best predictors. Here we see that `X1` and `X2` are not important. Why?

```{r, echo = FALSE, fig.height = 6, fig.width = 6}
cor(reg_trn[, 2:7])
```

It seems that `X3` and `X4` are highly correlated with `X1` and `X2` and must not add much in terms of prediction.

As a result, we will consider some models that use only `X3`, `X4`, `X6`, and `X22`.

## Methods

We will consider four methods:

- Random Forest (`rf`)
- Linear Regression (`lm`)
- Penalized Linear Regression, Elastic-Net (`glmnet`)
- Generalized Additive Models (`gam`)

For each method we will consider different sets of features:

- `small`: Only `X3`, `X4`, `X6`, and `X22`
- `int`: All possible interactions between `X3`, `X4`, `X6`, and `X22`
- `full`: All features.
- `huge`: All features with all possible two way interactions.

```{r}
small_form = formula(y ~ X3 + X4 + X6 + X22)
int_form   = formula(y ~ X3 * X4 * X6 * X22)
full_form  = formula(y ~ .)
huge_form  = formula(y ~ . ^ 2)
```

`huge` will only be considered with Penalized Logistic Regression since it is a very large number of features.

```{r}
cv_5 = trainControl(method = "cv", number = 5, verbose = FALSE)
```

Tuning will be done using 5-fold cross-validation. (We could use OOB metrics for Random Forests, but here we sacrifice computation time for better comparison.)

```{r, echo = FALSE}
set.seed(uin)
```

```{r}
rf_small = train(small_form, data = reg_trn, method = "rf",
                 trControl = cv_5, tuneLength = 2)

rf_int   = train(int_form, data = reg_trn, method = "rf",
                 trControl = cv_5)

rf_full  = train(full_form, data = reg_trn, method = "rf",
                 trControl = cv_5)
```

Random Forests are fit above using `caret` with `method = "rf"`. For most we use the default training grid, which considers a low, medium and high `mtry`. We specify a training grid of length `2` for the small feature set to avoid a warning.

```{r}
lm_small = train(small_form, data = reg_trn, method = "lm", trControl = cv_5)

lm_int   = train(int_form, data = reg_trn, method = "lm", trControl = cv_5)

lm_full  = train(full_form, data = reg_trn, method = "lm", trControl = cv_5)
```

Linear Regressions are fit above using `caret` with `method = "lm"`. There is nothing to tune, but we store the results for comparing cross-validated errors later.

```{r}
glmn_small = train(small_form, data = reg_trn, method = "glmnet",
                   trControl = cv_5, tuneLength = 10)

glmn_int   = train(int_form, data = reg_trn, method = "glmnet",
                   trControl = cv_5, tuneLength = 10)

glmn_full  = train(full_form, data = reg_trn, method = "glmnet",
                   trControl = cv_5, tuneLength = 10)

glmn_huge  = train(huge_form, data = reg_trn, method = "glmnet",
                   trControl = cv_5, tuneLength = 10)
```

Penalized Linear Regressions are fit above using `caret` with `method = "glmnet"`. We simply specify a tuning grid length. `caret` will try 10 $\alpha$ values and a number of $\lambda$ values for each.

```{r}
gam_grid =  expand.grid(df = seq(1, 5, by = 0.5))

gam_small  = train(y ~ X3 + X4 + X6 + X22, data = reg_trn, method = "gamSpline",
                  verbose = FALSE, trControl = cv_5, tuneGrid = gam_grid)

gam_full  = train(y ~ ., data = reg_trn, method = "gamSpline",
                  verbose = FALSE, trControl = cv_5, tuneGrid = gam_grid)
```

Generalized Additive Models are fit above using `caret` with `method = "gamSpline"`. We specify a number of degrees of freedom to try. Note that our ability to tune these models with `caret` is very limited. (For example, each term must use the same degrees of freedom and we cannot add parametric terms.)

## Results

```{r, echo = FALSE}
model_list_reg = list(
  rf_small = rf_small,
  rf_int = rf_int,
  rf_full = rf_full,
  lm_small = lm_small,
  lm_int = lm_int,
  lm_full = lm_full,
  glmn_small = glmn_small,
  glmn_int = glmn_int,
  glmn_full = glmn_full,
  glmn_huge = glmn_huge,
  gam_small = gam_small,
  gam_full = gam_full
)
```

```{r, echo = FALSE}
cv_rmse = rep(0, length(model_list_reg))
for (i in seq_along(model_list_reg)) {
  cv_rmse[i] = min(model_list_reg[[i]]$results$RMSE)
}

sd_cv_rmse = rep(0, length(model_list_reg))
for (i in seq_along(model_list_reg)) {
  sd_cv_rmse[i] = model_list_reg[[i]]$results$RMSESD[which.min(model_list_reg[[i]]$results$RMSE)]
}
```

```{r, echo = FALSE}
results_reg = data.frame(models = names(model_list_reg),
                         cv_rmse = cv_rmse, 
                         sd_cv_rmse = sd_cv_rmse)
colnames(results_reg) = c("Method + Features", "CV RMSE", "SD CV RMSE")
results_reg = results_reg[order(results_reg$`CV RMSE`), ]
rownames(results_reg) = NULL

kable(results_reg, format = "html", digits = 3) %>%
  kable_styling(bootstrap_options = "striped", full_width = FALSE) %>%
  row_spec(row = 2, bold = TRUE, color = "white", background = "lightgreen")
```

## Discussion

Based on the above cross-validated RMSEs, our choice is somewhat obvious. `lm_int` has both the lowest cross-validated RMSE and a reasonably low variability. However, `glmn_int` has very similar performance, and actually uses fewer predictors, so we give it a slight preference.

```{r}
glmn_int$bestTune
```

```{r}
coef(glmn_int$finalModel, s = glmn_int$bestTune$lambda)
```

Since this was synthetic data, we can actually see the true data generating process.

```{r}
# define the data generating process
gen_reg_data = function(n) {
  p = 20
  mu = c(1, 2, 3, 4)
  sigma = matrix(.9, nrow = 4, ncol = 4) + diag(4) * .3
  x_corr = mvrnorm(n, mu = mu, Sigma = sigma)
  x_uncorr = replicate(p - 4, rnorm(n, sample(1:(p - 4)), 3))
  x_cat_1 = sample(c("A", "B", "C"), n, replace = TRUE)
  x_cat_2 = sample(c("D", "E", "F"), n, replace = TRUE)
  x = cbind(x_corr, x_uncorr)
  x = data.frame(round(x, 3))
  x = cbind(x, X21 = x_cat_1, X22 = x_cat_2)
  y = round(2 * x[, 3] + x[, 4] + 2 * x[, 4] * (x[, 22] == "D") - 1 * x[, 6] + rnorm(n, 0, 1.5), 3)
  data.frame(y, x)
}
```

As a result, the following function is the best possible model for this data.

```{r}
f = function(predictors) {
  2 * predictors$X3 + predictors$X4 + 2 * predictors$X4 * (predictors$X22 == "D") - 1 * predictors$X6
}
```

```{r}
reg_tst_y = read_csv("reg-tst-y.csv") # this data not released
```

Thus the RMSE below is the best we could achieve on the test data.

```{r}
rmse(f(reg_tst), reg_tst_y$y)
```

However, in general a RMSE of 1.5 is what we would hope to achieve, as that is how noise was added to the data. The method we used gets very close.

While `lm_int` performs best in terms of CV-RMSE, it turns on that `glmn_int` is closer to the true model as it sets some of the unneeded predictors of `lm_int` to be zero. In the test set, `glmn_int` performs slightly better.

```{r}
rmse(predict(lm_int, reg_tst), reg_tst_y$y)
```

```{r}
rmse(predict(glmn_int, reg_tst), reg_tst_y$y)
```

***

# Spam Filter

## Introduction

```{r, message = FALSE, warning = FALSE}
spam_trn = read_csv("https://daviddalpiaz.github.io/stat432sp18/hw/hw08/spam-trn.csv")
spam_tst = read_csv("https://daviddalpiaz.github.io/stat432sp18/hw/hw08/spam-tst.csv")
```

```{r}
spam_trn$type = factor(spam_trn$type)
```

```{r}
score = function(actual, predicted) {
  1   * sum(predicted == "spam" & actual == "spam") +
  -25 * sum(predicted == "spam" & actual == "nonspam") +
  -1  * sum(predicted == "nonspam" & actual == "spam") +
  2   * sum(predicted == "nonspam" & actual == "nonspam")
}
```

## Methods

For this problem, we simply cut to the chase, and utilize a random forest. This time, we do split off some "test" data.

```{r}
set.seed(uin)
spam_trn_idx = createDataPartition(spam_trn$type, p = 0.80, list = FALSE)
spam_trn_trn = spam_trn[spam_trn_idx, ]
spam_trn_tst = spam_trn[-spam_trn_idx, ]
```

```{r}
# tune for best accuracy, use OOB metrics
rf_mod_spamn_acc = train(type ~ ., data = spam_trn_trn,
                         method = "rf",
                         trControl = trainControl(method = "oob"))
```

```{r}
# tune for best ROC, caret does not allow OOB, so we use CV
rf_mod_spamn_roc = train(type ~ ., data = spam_trn_trn,
                         method = "rf",
                         trControl = trainControl(method = "cv",
                                                  classProbs = TRUE,
                                                  summaryFunction = twoClassSummary),
                         metric = "ROC")
```

```{r}
cutoffs = seq(0.5, 1, by = 0.01)
score_acc = rep(0, length(cutoffs))
score_roc = rep(0, length(cutoffs))

# test cutoffs for accuracy tuned forest
for (c in seq_along(cutoffs)) {
  pred = ifelse(predict(rf_mod_spamn_acc, spam_trn_tst, type = "prob")$spam > cutoffs[c], 
                "spam", "nonspam")
  score_acc[c] = score(predicted = pred,
                       actual = spam_trn_tst$type)
}

# test cutoffs for ROC tuned forest
for (c in seq_along(cutoffs)) {
  pred = ifelse(predict(rf_mod_spamn_roc, spam_trn_tst, type = "prob")$spam > cutoffs[c], 
                "spam", "nonspam")
  score_roc[c] = score(predicted = pred,
                       actual = spam_trn_tst$type)
}
```

## Results

```{r, echo = FALSE}
plot(cutoffs, score_acc, type = "l", col = "dodgerblue", main = "Score vs Cutoff",
     xlab = "cutoff", ylab = "score", lty = 1,
     ylim = c(min(c(score_acc, score_roc)), max(c(score_acc, score_roc))))
lines(cutoffs, score_roc, type = "l", col = "darkorange", lty = 2)
legend("bottomleft", c("Accuracy Tuned", "ROC Tuned"), lty = c(1, 2), 
       lwd = 1, col = c("dodgerblue", "darkorange"))
```

We see that tuning with ROC performs ever so slightly better. (It almost doesn't seem like it was worth it to spend the extra time to tune that model.) As expected, we need to increase the spam threshold.

```{r}
(cut_to_use = cutoffs[which.max(score_roc)])
```

## Discussion

It is certainly possible that with more thought we could discover a better filter, but this analysis uses the key concept, modifying the cutoffs for classifying.

```{r}
spam_tst_y = read_csv("spam_tst-y.csv") # data not provided
spam_tst_y$type = factor(spam_tst_y$type)
```

```{r}
pred_prob = predict(rf_mod_spamn_roc, spam_tst, type = "prob")$spam
pred_class = ifelse(pred_prob > cut_to_use, "spam", "nonspam")
score(actual = spam_tst_y$type, predicted = pred_class) # score on autograder
score(actual = spam_tst_y$type, predicted = predict(rf_mod_spamn_roc, spam_tst))
```

```{r}
accuracy(actual = spam_tst_y$type, predicted = pred_class)
accuracy(actual = spam_tst_y$type, predicted = predict(rf_mod_spamn_roc, spam_tst))
```

As expected, we see a decrease in accuracy, for an increase in score.

```{r}
varImpPlot(rf_mod_spamn_roc$finalModel, n.var = 10)
```

Looking at the ten most important variables, we see things like exclamation marks, capital letters, and dollar signs are good predictors. Remember, this is an **old** dataset, from HP employees. (Note that `hp` also appears to be important.) This would be a terrible spam classifier for general email data today.

***

# Miscellaneous Notes

- We're loosing using the IMRD style here, but this document doesn't serve as a good template for project reports.
    - For example, the methods sections contain too many specifics about computing that would be unnecessary for project reports.
    - The introductions are lacking a lot of background. (We can get away with this here since we are all familiar with the assignment.)
- We could have, and perhaps should have, split off some of the training data to be used as a test dataset. This would have been absolutely necessary if we used OOB metrics to tune the random forests.
