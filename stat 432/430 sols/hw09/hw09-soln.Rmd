---
title: "Homework 09"
author: "STAT 430, Fall 2017"
date: 'Due: Monday, November 20, 11:59 PM'
urlcolor: cyan
---

```{r options, include = FALSE}
knitr::opts_chunk$set(fig.align = "center")
```

***

# Exercise 1 (Computation Time)

**[8 points]** For this exercise we will create data via simulation, then assess how well certain methods perform. Use the code below to create a train and test dataset.

```{r, message = FALSE, warning = FALSE}
library(mlbench)
set.seed(42)
sim_trn = mlbench.spirals(n = 2500, cycles = 1.5, sd = 0.125)
sim_trn = data.frame(sim_trn$x, class = as.factor(sim_trn$classes))
sim_tst = mlbench.spirals(n = 10000, cycles = 1.5, sd = 0.125)
sim_tst = data.frame(sim_tst$x, class = as.factor(sim_tst$classes))
```

The training data is plotted below, with colors indicating the `class` variable, which is the response.

```{r, fig.height = 5, fig.width = 5, echo = FALSE}
sim_trn_col = ifelse(sim_trn$class == 1, "darkorange", "dodgerblue")
plot(sim_trn$X1, sim_trn$X2, col = sim_trn_col,
     xlab = "X1", ylab = "X2", pch = 20)
```

Before proceeding further, set a seed equal to your UIN.

```{r}
uin = 123456789
set.seed(uin)
```

We'll use the following to define 5-fold cross-validation for use with `train()` from `caret`.

```{r, message = FALSE, warning = FALSE}
library(caret)
cv_5 = trainControl(method = "cv", number = 5)
```

We now tune two models with `train()`. First, a logistic regression using `glm`. (This actually isn't "tuned" as there are not parameters to be tuned, but we use `train()` to perform cross-validation.) Second we tune a single decision tree using `rpart`.

We store the results in `sim_glm_cv` and `sim_tree_cv` respectively, but we also wrap both function calls with `system.time()` in order to record how long the tuning process takes for each method.

```{r, message = FALSE, warning = FALSE}
glm_cv_time = system.time({
  sim_glm_cv  = train(
    class ~ .,
    data = sim_trn,
    trControl = cv_5,
    method = "glm")
})

tree_cv_time = system.time({
  sim_tree_cv = train(
    class ~ .,
    data = sim_trn,
    trControl = cv_5,
    method = "rpart")
})
```

We see that both methods are tuned via cross-validation in a similar amount of time.

```{r}
glm_cv_time["elapsed"]
tree_cv_time["elapsed"]
```

```{r, message = FALSE, warning = FALSE, echio = FALSE}
library(rpart.plot)
rpart.plot(sim_tree_cv$finalModel)
```

Repeat the above analysis using a random forest, twice. The first time use 5-fold cross-validation. (This is how we had been using random forests before we understood random forests.) The second time, tune the model using OOB samples. We only have two predictors here, so, for both, use the following tuning grid.

```{r}
rf_grid = expand.grid(mtry = c(1, 2))
```

```{r, message = FALSE, warning = FALSE, solution = TRUE}
oob  = trainControl(method = "oob")
rf_oob_time = system.time({
  sim_rf_oob = train(
    class ~ .,
    data = sim_trn,
    trControl = oob,
    tuneGrid = rf_grid)
})

rf_cv_time = system.time({
  sim_rf_cv = train(
    class ~ .,
    data = sim_trn,
    trControl = cv_5,
    tuneGrid = rf_grid)
})
```

Create a table summarizing the results of these four models. (Logistic with CV, Tree with CV, RF with OOB, RF with CV). Report:

- Chosen value of tuning parameter (If applicable)
- Elapsed tuning time
- Resampled (CV or OOB) Accuracy
- Test Accuracy

```{r, solution = TRUE, echo = FALSE}
best_tune = c(NA, sim_tree_cv$bestTune$cp, sim_rf_oob$bestTune$mtry, sim_rf_cv$bestTune$mtry)

time_results = c(glm_cv_time["elapsed"], 
                 tree_cv_time["elapsed"], 
                 rf_oob_time["elapsed"], 
                 rf_cv_time["elapsed"])

resampled_acc = c(max(sim_glm_cv$results$Accuracy),
                  max(sim_tree_cv$results$Accuracy),
                  max(sim_rf_oob$results$Accuracy), 
                  max(sim_rf_cv$results$Accuracy))

calc_acc = function(actual, predicted) {
  mean(actual == predicted)
}

glm_cv_tst_acc = calc_acc(predicted = predict(sim_glm_cv, sim_tst),
                          actual    = sim_tst$class)

tree_cv_tst_acc = calc_acc(predicted = predict(sim_tree_cv, sim_tst),
                           actual    = sim_tst$class)

rf_cv_tst_acc = calc_acc(predicted = predict(sim_rf_cv, sim_tst),
                         actual    = sim_tst$class)

rf_oob_tst_acc = calc_acc(predicted = predict(sim_rf_oob, sim_tst),
                          actual    = sim_tst$class)

test_acc = c(glm_cv_tst_acc, tree_cv_tst_acc, rf_cv_tst_acc, rf_oob_tst_acc)
```

```{r, solution = TRUE, echo = FALSE}
timing_results = data.frame(
  c("Logistic, CV", "Tree, CV", "RF, OOB", "RF, CV"),
  best_tune,
  time_results,
  resampled_acc,
  test_acc
)
colnames(timing_results) = c("Method", "Best Tune", "Elapsed", "Resampled Accuracy", "Test Accuracy")
knitr::kable(timing_results, digits = 4)
```


# Exercise 2 (Predicting Baseball Salaries)

**[7 points]** For this question we will predict the `Salary` of `Hitters`. (`Hitters` is also the name of the dataset.) We first remove the missing data:

```{r}
library(ISLR)
Hitters = na.omit(Hitters)
```

After changing `uin` to your UIN, use the following code to test-train split the data.

```{r}
uin = 123456789
set.seed(uin)
hit_idx = createDataPartition(Hitters$Salary, p = 0.6, list = FALSE)
hit_trn = Hitters[hit_idx,]
hit_tst = Hitters[-hit_idx,]
```

Do the following:

- Tune a boosted tree model using the following tuning grid and 5-fold cross-validation.

```{r}
gbm_grid = expand.grid(interaction.depth = c(1, 2),
                       n.trees = c(500, 1000, 1500),
                       shrinkage = c(0.001, 0.01, 0.1),
                       n.minobsinnode = 10)
```

- Tune a random forest using OOB resampling and **all** possible values of `mtry`. 

Create a table summarizing the results of three models:

- Tuned boosted tree model
- Tuned random forest model
- Bagged tree model

For each, report:

- Resampled RMSE
- Test RMSE

```{r, solution = TRUE, message = FALSE, warning = FALSE}
hit_gbm = train(Salary ~ ., data = hit_trn,
                method = "gbm",
                trControl = cv_5,
                verbose = FALSE,
                tuneGrid = gbm_grid)

rf_grid = rf_grid = expand.grid(mtry = 1:(ncol(hit_trn) - 1))
hit_rf  = train(Salary ~ ., data = hit_trn,
                method = "rf",
                trControl = oob,
                tuneGrid = rf_grid)

# storing the bagged model for making predictions
hit_bag = train(Salary ~ ., data = hit_trn,
                method = "rf",
                trControl = oob,
                tuneGrid = data.frame(mtry = (ncol(hit_trn) - 1)))
```

```{r, solution = TRUE}
calc_rmse = function(actual, predicted) {
  sqrt(mean((actual - predicted) ^ 2))
}

gbm_tst_rmse = calc_rmse(predicted = predict(hit_gbm, hit_tst),
                         actual    = hit_tst$Salary)

rf_tst_rmse = calc_rmse(predicted = predict(hit_rf, hit_tst),
                        actual    = hit_tst$Salary)

bag_tst_rmse = calc_rmse(predicted = predict(hit_bag, hit_tst),
                         actual    = hit_tst$Salary)


hitters_results = data.frame(
  c("Boosting", "Random Forest", "Bagging"),
  c(min(hit_gbm$results$RMSE), min(hit_rf$results$RMSE), min(hit_bag$results$RMSE)),
  c(gbm_tst_rmse, rf_tst_rmse, bag_tst_rmse)
)
colnames(hitters_results) = c("Method", "Resampled RMSE", "Test RMSE")
knitr::kable(hitters_results, digits = 4)
```


# Exercise 3 (Transforming the Response)

**[5 points]** Continue with the data from Exercise 2. The book, ISL, suggests log transforming the response, `Salary`, before fitting a random forest. Is this necessary? Re-tune a random forest as you did in Exercise 2, except with a log transformed response. Report test RMSE for both the untransformed and transformed model on the original scale of the response variable. 

```{r, echo = FALSE}
histogram(hit_trn$Salary, xlab = "Salaray")
```


```{r, solution = TRUE}
hit_rf_log  = train(log(Salary) ~ ., data = hit_trn,
                    method = "rf",
                    trControl = oob,
                    tuneGrid = rf_grid)
```

```{r, solution = TRUE}
# without transformation
calc_rmse(predicted = predict(hit_rf, hit_tst),
          actual    = hit_tst$Salary)

# with log transformation
calc_rmse(predicted = exp(predict(hit_rf_log, hit_tst)),
          actual = hit_tst$Salary)
```


# Exercise 4 (Concept Checks)

**[1 point each]** Answer the following questions based on your results from the three exercises. 

### Timing

**(a)** Compare the time taken to tune each model. Is the difference between the OOB and CV result for the random forest similar to what you would have expected?

**Solution:** The speed-up for OOB is only about three times that of 5-fold CV, instead of the five times that would have been expected. There appears to be some additional overhead in using OOB.

```{r, solution = TRUE}
rf_cv_time["elapsed"] / rf_oob_time["elapsed"]
```

**(b)** Compare the tuned value of `mtry` for each of the random forests tuned. Do they choose the same model?

**Solution:** They choose the same model, although, there were only two to choose from, and they are not very different. In practice, the two methods may differ more.

**(c)** Compare the test accuracy of each of the four procedures considered. Briefly explain these results.

**Solution:** 
  
- Logistic: Performs the worst. This is expected as clearly a non-linear decision boundary is needed.
- Single Tree: Better than logistic, but not the best seen here. We see above that this is not a very deep tree. It will have non-linear boundaries, but since it uses binary splits, they will be rectangular regions.
- Random Forest: First note that both essentially fit the same model. (The exact forests will be different due to randomization.) By using many trees (500) the boundaries will become less rectangular than the single tree, and will better match the spiral data in the data.

- See below for plots of decision boundaries created by making predictions from the different models.

```{r, fig.height = 5, fig.width = 13, solution = TRUE, echo = FALSE}
plot_grid = expand.grid(
  X1 = seq(min(sim_tst$X1) - 1, max(sim_tst$X1) + 1, by = 0.01),
  X2 = seq(min(sim_tst$X2) - 1, max(sim_tst$X2) + 1, by = 0.01)
)

glm_pred  = predict(sim_glm_cv, plot_grid)
tree_pred = predict(sim_tree_cv, plot_grid)
rf_pred   = predict(sim_rf_oob, plot_grid)

glm_col = ifelse(glm_pred == 1, "darkorange", "dodgerblue")
tree_col = ifelse(tree_pred == 1, "darkorange", "dodgerblue")
rf_col = ifelse(rf_pred == 1, "darkorange", "dodgerblue")

par(mfrow = c(1, 3))
plot(plot_grid$X1, plot_grid$X2, col = glm_col,
     xlab = "X1", ylab = "X2", pch = 20, main = "Logistic Regression",
     xlim = c(-1.5, 1.5), ylim = c(-1.5, 1.5))
plot(plot_grid$X1, plot_grid$X2, col = tree_col,
     xlab = "X1", ylab = "X2", pch = 20, main = "Single Tree",
     xlim = c(-1.5, 1.5), ylim = c(-1.5, 1.5))
plot(plot_grid$X1, plot_grid$X2, col = rf_col,
     xlab = "X1", ylab = "X2", pch = 20, main = "Random Forest",
     xlim = c(-1.5, 1.5), ylim = c(-1.5, 1.5))
```

### Salary

**(d)** Report the tuned value of `mtry` for the random forest.

```{r, solution = TRUE}
hit_rf$bestTune
```

**(e)** Create a plot that shows the tuning results for the tuning of the boosted tree model.

```{r, solution = TRUE}
plot(hit_gbm)
```

**(f)** Create a plot of the variable importance for the tuned random forest.

```{r, solution = TRUE, echo = FALSE}
varImpPlot(hit_rf$finalModel, main = "Variable Importance, Random Forest")
```

**(g)** Create a plot of the variable importance for the tuned boosted tree model.

```{r, solution = TRUE, echo = FALSE}
plot(varImp(hit_gbm), main = "Variable Importance, Boosting")
```

**(h)** According to the random forest, what are the three most important predictors?

```{r, solution = TRUE, echo = FALSE}
names(importance(hit_rf$finalModel)[order(importance(hit_rf$finalModel), decreasing = TRUE), ][1:3])
```

**(i)** According to the boosted model, what are the three most important predictors?

```{r, solution = TRUE, echo = FALSE}
rownames(varImp(hit_gbm)$importance)[order(varImp(hit_gbm)$importance$Overall, decreasing = TRUE)][1:3]
```

### Transformation

**(j)** Based on these results, do you think the transformation was necessary?

**Solution:**  Here we see that the untransformed model actually performs better. However, they are relatively close, so either could be acceptable. Note that a random forest can model a non-linear relationship, which is why the transformation is not necessary.
